{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ridge_and_Lasso_Regularization_Methods_in_Linear_Regression.ipynb",
      "provenance": [],
      "mount_file_id": "1qTbsGw0rro61XaLxmEcFbifNVpPHr1uz",
      "authorship_tag": "ABX9TyM+5FnRb9PX4GOAmLceNZI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciscosalido/AIML/blob/master/Ridge_and_Lasso_Regularization_Methods_in_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QccmNqQxlZWX",
        "colab_type": "text"
      },
      "source": [
        "# **Regularization Method (Shrinkage Method)**\n",
        "\n",
        "A lot of variables and too much parameters in a rich dimension dataset exposes us to the **Curse of Dimensionality** where the number of observations is not sufficient to cover all possibilities of combinations between the independent variables.\n",
        "\n",
        "We can resort to dimensionality reduction techniques such as transforming to PCA and eliminating the PCA with least magnitude of eigenvalues. This cam be a laborious process before we find the right number of important components.\n",
        "\n",
        "Instead we can emplay the Regularising / shrinkage Methods.\n",
        "\n",
        "Regularisation helps us to deal with the problem of overfitting by reducing the weight given to a particular feature *x*. This allows us to retain more features while not giving undue weight to one in particular.\n",
        "\n",
        " Regularisation is mediated by a parameter λ, as can be seen in the cost function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9reT-1uL9xK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "$$J(\\theta)=\\frac{1}{2m}\\Big(\\sum^{m}_{i=1}(h_{\\theta}(x^{(i)}-y^{(i)})^2\\Big)+\\frac{\\lambda}{2m}\\Big(\\sum^{n}_{j=1}\\theta^2_j\\Big)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePyBFWVwMfck",
        "colab_type": "text"
      },
      "source": [
        "The first term is essentially the mean-squared-error term, whilst the additive term multiplies the sum of the square of the parameters (θ) by λ over 2m, where m is the number of training examples. \n",
        "\n",
        " **The Penalty or Lambda Factor (λ) is the tuning parameter that decides how much we want to penalize the flexibility of our model.**\n",
        "\n",
        "Since the objective is to minimise J(θ) (minθJ(θ)) using a large λ will require small values of θj in order to achive a minimum value.\n",
        "\n",
        "The increase in flexibility of a model is represented by increase in its coefficients, and if we want to minimize the above function, then these coefficients need to be small. This is how the Ridge regression technique prevents coefficients from rising too high. Also, notice that we shrink the estimated association of each variable with the response, except the intercept β0, This intercept is a measure of the mean value of the response when xi1 = xi2 = …= xip = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAv3XT0sif8V",
        "colab_type": "text"
      },
      "source": [
        "## Regression:\n",
        "\n",
        "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, **this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.**\n",
        "\n",
        "A simple relation for linear regression looks like this. Here Y represents the learned relation and β represents the coefficient estimates for different variables or predictors(X).\n",
        "\n",
        "$$Y ≈ β0 + β1X1 + β2X2 + …+ βpXp$$\n",
        "\n",
        "The fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/908/1*DY3-IaGcHjjLg7oYXx1O3A.png)\n",
        "\n",
        "Now, this will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients won’t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OGkFK05WqJe",
        "colab_type": "text"
      },
      "source": [
        "### 1. Ridge Regression(Tikhonov Regularization) \n",
        "\n",
        "is similar to the linear regression where the objective is to find the best fit surface. The difference is in the way the best coefficients are found. Unlike linear regression where the optimization fuction is SSE, in the Ridge Regression it is slighty different.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1106/1*CiqZ8lhwxi5c4d1nV24w4g.png)\n",
        "\n",
        "Above image shows ridge regression, where the RSS is modified by adding the shrinkage quantity. Now, the coefficients are estimated by minimizing this function. Here, λ is the tuning parameter that decides how much we want to penalize the flexibility of our model. The increase in flexibility of a model is represented by increase in its coefficients, and if we want to minimize the above function, then these coefficients need to be small. This is how the Ridge regression technique prevents coefficients from rising too high. Also, notice that we shrink the estimated association of each variable with the response, except the intercept β0, This intercept is a measure of the mean value of the response when xi1 = xi2 = …= xip = 0.\n",
        "When λ = 0, the penalty term has no eﬀect, and the estimates produced by ridge regression will be equal to least squares. However, as λ→∞, the impact of the shrinkage penalty grows, and the ridge regression coeﬃcient estimates will approach zero. As can be seen, selecting a good value of λ is critical. Cross validation comes in handy for this purpose. The coefficient estimates produced by this method are also known as the L2 norm.\n",
        "The coefficients that are produced by the standard least squares method are scale equivariant, i.e. if we multiply each input by c then the corresponding coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor is scaled, the multiplication of predictor and coefficient(Xjβj) remains the same. However, this is not the case with ridge regression, and therefore, we need to standardize the predictors or bring the predictors to the same scale before performing ridge regression. The formula used to do this is given below.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/562/1*6KRAdbf-CApFPR7gASZaSA.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAttVxX1htQh",
        "colab_type": "text"
      },
      "source": [
        "### Lasso Regression \n",
        "\n",
        "![alt text](https://miro.medium.com/max/1094/1*tHJ4sSPYV0bDr8xxEdiwXA.png)\n",
        "\n",
        "Lasso is another variation, in which the above function is minimized. Its clear that this variation differs from ridge regression only in penalizing the high coefficients. It uses |βj|(modulus)instead of squares of β, as its penalty. In statistics, this is known as the L1 norm.\n",
        "Lets take a look at above methods with a different perspective. The ridge regression can be thought of as solving an equation, where summation of squares of coefficients is less than or equal to s. And the Lasso can be thought of as an equation where summation of modulus of coefficients is less than or equal to s. Here, s is a constant that exists for each value of shrinkage factor λ. These equations are also referred to as constraint functions.\n",
        "Consider their are 2 parameters in a given problem. Then according to above formulation, the ridge regression is expressed by β1² + β2² ≤ s. This implies that ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by β1² + β2² ≤ s.\n",
        "Similarly, for lasso, the equation becomes,|β1|+|β2|≤ s. This implies that lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by |β1|+|β2|≤ s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqKdL2W_k6xm",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZg3DlFzjq9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8d96d221-0d44-47e6-bfc3-3a98c0e7d423"
      },
      "source": [
        "import io\n",
        "import numpy as np # Import Numerical Linear Algebra library\n",
        "import pandas as pd # Import Data Procession library\n",
        "import matplotlib.pyplot as plt # Import the Pyplot instance from Matplotlib library\n",
        "import seaborn as sns # Import a Statistical Data Visualization library\n",
        "\n",
        "# Set matplotlib to automatic print any plot\n",
        "%matplotlib inline "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y061OlzbvOOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import some Preprocessing Libraries\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import Linear Regression Machine Learning Libray\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gw2RLBUlKvZ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSN-9YErkppl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ccbc3892-0122-4405-a19d-76528a201a09"
      },
      "source": [
        "# Mount the google drive and set the path to the datasets\n",
        "from google.colab import drive\n",
        "drive.mount(r'/content/drive', force_remount=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yY2IhvIk0Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = r'/content/drive/My Drive/AIML/'  #set a dir to the project folder\n",
        "!ls /content/drive/My\\ Drive/AIML/. #change dir to the project folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK1q-SWKk2Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mpg_df = pd.read_csv(r'/content/drive/My Drive/AIML/car-mpg.csv')\n",
        "mpg_df = mpg_df.drop('car_name', axis= 1)\n",
        "mpg_df['origin'] = mpg_df['origin'].replace({1: 'america', 2: 'europe', 3: 'asia'})\n",
        "mpg_df = pd.get_dummies(mpg_df, columns= ['origin'])\n",
        "mpg_df = mpg_df.replace('?', np.nan)\n",
        "mpg_df = mpg_df.apply(lambda x: x.fillna(x.median()), axis= 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFfe_bouw0yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "595dd33f-115f-462f-a750-94e54bb9cc5a"
      },
      "source": [
        "mpg_df.info()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 398 entries, 0 to 397\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   mpg             398 non-null    float64\n",
            " 1   cyl             398 non-null    int64  \n",
            " 2   disp            398 non-null    float64\n",
            " 3   hp              398 non-null    object \n",
            " 4   wt              398 non-null    int64  \n",
            " 5   acc             398 non-null    float64\n",
            " 6   yr              398 non-null    int64  \n",
            " 7   car_type        398 non-null    int64  \n",
            " 8   origin_america  398 non-null    uint8  \n",
            " 9   origin_asia     398 non-null    uint8  \n",
            " 10  origin_europe   398 non-null    uint8  \n",
            "dtypes: float64(3), int64(4), object(1), uint8(3)\n",
            "memory usage: 26.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4i_-ePtxWO8",
        "colab_type": "text"
      },
      "source": [
        "### Separate Independent and Dependent Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWGSZNv0xSS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy all the predictor variables into 'df' dataframe. Since 'mpg' is the dependent variable we will drop it\n",
        "df = mpg_df.drop('mpg', axis= 1)\n",
        "y = mpg_df['mpg']\n",
        "\n",
        "#scale all the columns of the mpg_df. This will produce a numpy array\n",
        "df_scaled = scale(df)\n",
        "df_scaled = pd.DataFrame(df_scaled, columns= df.columns) # ideally the training and test should be"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTKZuZ4expe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y, test_size= 0.30, random_state= 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhp9txeE0OHu",
        "colab_type": "text"
      },
      "source": [
        "### Fit a Simple Linear Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KoCtTDV0EGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "cd4a0bd4-90b5-408a-87b8-b6ca511bb840"
      },
      "source": [
        "regression_model = LinearRegression()\n",
        "regression_model.fit(X_train, y_train)\n",
        "\n",
        "for idx, col_name in enumerate(X_train.columns):\n",
        "  print('The coefficient for {} is {}'.format(col_name, regression_model.coef_[idx]))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient for cyl is 2.5059518049385026\n",
            "The coefficient for disp is 2.5357082860560514\n",
            "The coefficient for hp is -1.7889335736325294\n",
            "The coefficient for wt is -5.551819873098727\n",
            "The coefficient for acc is 0.11485734803440747\n",
            "The coefficient for yr is 2.9318465482116087\n",
            "The coefficient for car_type is 2.977869737601945\n",
            "The coefficient for origin_america is -0.583295529016598\n",
            "The coefficient for origin_asia is 0.34749313804322646\n",
            "The coefficient for origin_europe is 0.3774164680868858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL0is2912TNK",
        "colab_type": "text"
      },
      "source": [
        "### Create a Regularized RIDGE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcriC1iD0xtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e163c851-3e0b-4d26-b609-f1e536b90f34"
      },
      "source": [
        "ridge = Ridge(alpha= 0.3) # alpha = lambda or penalty factor\n",
        "ridge.fit(X_train, y_train)\n",
        "print('Ridge Model Coefficients are {}'.format(ridge.coef_))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge Model Coefficients are [ 2.47057467  2.44494419 -1.78573889 -5.47285499  0.10115618  2.92319984\n",
            "  2.94492098 -0.57949986  0.34667456  0.37344909]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju5M2YJJ3MjW",
        "colab_type": "text"
      },
      "source": [
        "### Create a regularized LASSO Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVv0azbO3LD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0e9cda75-9892-4edc-8038-a2c6e3d82a6c"
      },
      "source": [
        "lasso = Lasso(alpha= 0.1) # alpha = lambda or penalty factor\n",
        "lasso.fit(X_train, y_train)\n",
        "print('Lasso Model Coefficients are {}'.format(lasso.coef_))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso Model Coefficients are [ 1.10693517  0.         -0.71587138 -4.2127655  -0.          2.73245903\n",
            "  1.66333749 -0.63587683  0.          0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYT-DuzN3sUG",
        "colab_type": "text"
      },
      "source": [
        "##### Observe that many of the coefficients have become 0 indicating drop of those dimensions from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUYxVJzi35N7",
        "colab_type": "text"
      },
      "source": [
        "### Score Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AWFWtyQ4HlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3176ad9e-bc4d-4953-8ffc-539f63db5008"
      },
      "source": [
        "print('Linear Regression Model Train Data Score {}'.format (regression_model.score(X_train, y_train)))\n",
        "print('Linear Regression Model Test Data Score {}'.format (regression_model.score(X_test, y_test)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Regression Model Train Data Score 0.8343770256960538\n",
            "Linear Regression Model Test Data Score 0.8513421387780067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6m-4dcr4sqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ce47e352-9ce0-48c4-f13f-ac0c4fcc89f2"
      },
      "source": [
        "print('Ridge Regulated Regression Model Train Data Score {}'.format (ridge.score(X_train, y_train)))\n",
        "print('Ridge Regulated Regression Model Test Data Score {}'.format (ridge.score(X_test, y_test)))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge Regulated Regression Model Train Data Score 0.8343617931312616\n",
            "Ridge Regulated Regression Model Test Data Score 0.8518882171608504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7kWbzL94_rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2eb69c72-6d28-46d3-f9c2-555a09f0ce64"
      },
      "source": [
        "print('Lasso Regulated Regression Model Train Data Score {}'.format (lasso.score(X_train, y_train)))\n",
        "print('Lasso Regulated Regression Model Test Data Score {}'.format (lasso.score(X_test, y_test)))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lasso Regulated Regression Model Train Data Score 0.8211445134781438\n",
            "Lasso Regulated Regression Model Test Data Score 0.8577234201035426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gtaNzkx5I_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}